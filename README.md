# Speech-to-Text | Get transcription with speakers (OpenAI Whisper + NeMo Speaker Diarization)

## Overview
This notebook combines Whisper ASR capabilities with Voice Activity Detection (VAD) and Speaker Embedding to identify the speaker for each sentence in the transcription generated by Whisper. First, the vocals are extracted from the audio to increase the speaker embedding accuracy, then the transcription is generated using Whisper, then the timestamps are corrected and aligned using WhisperX to help minimize diarization error due to time shift. The audio is then passed into MarbleNet for VAD and segmentation to exclude silences, TitaNet is then used to extract speaker embeddings to identify the speaker for each segment, the result is then associated with the timestamps generated by WhisperX to detect the speaker for each word based on timestamps and then realigned using punctuation models to compensate for minor time shifts.


### Step 1: Clone the Repository

First, clone the repository to your local machine using the command:

```bash
git clone https://github.com/Abdulkadir19997/Speech_to_text_diarization.git
```
### Step 2: Create Python Environment

Inside the downloaded 'Automated-Inpaint-Anything' folder, create a Python environment, **I used 3.10.12 version of python**. For example, to create an environment named 'venv', use:

```bash
python -m venv venv
```

### Step 3: Activate Environment

Activate the environment with:

**For Windows**
```bash
.\venv\Scripts\activate
```

**For Linux**
```bash
source venv/bin/activate
```

### Step 4: Install Requirements

After confirming that the venv environment is active, install all necessary libraries from the 'requirements.txt' file:

```bash
pip install -r requirements.txt
```